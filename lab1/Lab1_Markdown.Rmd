---
title: "Lab1"
author: "Ludwig Thaung Elon Brange (ludth852, elobr959)"
date: '2019-04-01'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Task 1
Let y1, ..., yn|theta - Bern(theta), and assume that you have obtained a sample with s = 14 successes in n = 20 trials. Assume a Beta(alpha0, beta0) prior for theta and let alpha0 = beta0 = 2.

###a) Posterior theta|y - Bern(alpha0 + s, beta0 + f).
Verify graphically that the posterior mean and standard deviation converges to the true values as the number of random draws grows large.

```{r taks1a, echo = FALSE}
#library(LaplacesDemon)

  alpha = 2
  beta = 2
  s = 14
  num = 20
  f = num - s
  p = s/num
  
BetaRandomPosteriorPriorPlotandMeanStdComparison <- function(a,b,n,p, flag){
  
  xGrid <- seq(0.001, 0.999, by=0.001)
  normalizedLikelihood = dbeta(xGrid, 20*p+1, 20*(1-p)+1)
  prior = dbeta(xGrid, a, b)
  posterior = dbeta(xGrid, a+20*p, b+20*(1-p))
  posteriorAlpha = a+20*p
  posteriorBeta = b+20*(1-p)
  posteriorRandom <- rbeta(n, posteriorAlpha, posteriorBeta)
  maxDensity <- max(normalizedLikelihood, prior, posterior) # Use to make the y-axis high enough
  
  posteriorMean = posteriorAlpha/(posteriorAlpha + posteriorBeta)
  print(paste0("Posterior Mean GT: ", posteriorMean))
  print(paste0("ground truth std: ", sqrt(posteriorAlpha*posteriorBeta/(((posteriorAlpha+posteriorBeta)^2)*(posteriorAlpha+posteriorBeta+1)))))
  print(paste0("std: ", sd(posteriorRandom)))
  print(paste0("Mean: ", mean(posteriorRandom)))
  
  if (flag == TRUE){
      plot(xGrid, normalizedLikelihood, type = 'l', lwd = 3, col = "blue", xlim <- c(0,1), 
          ylim <- c(0, maxDensity), xlab = "theta", 
          ylab = 'Density', main = 'Bernoulli model - Beta(a,b) prior')
  lines(xGrid, posterior, lwd = 3, col = "red")
  lines(xGrid, prior, lwd = 3, col = "green")
  legend(x = 0.01, y = maxDensity*0.95, legend = c("Likelihood (normalized)", "Prior", "Posterior"), 
         col = c("blue","green","red"), lwd = c(3,3,3), cex = 0.7)    
  }

}

```
For 20 draws we get:

```{r, echo = FALSE}
BetaRandomPosteriorPriorPlotandMeanStdComparison(alpha, beta, num, p, FALSE)

```

For 10 0000 draws we get:
```{r, echo = FALSE}
BetaRandomPosteriorPriorPlotandMeanStdComparison(alpha, beta, 10000, p, TRUE)
```

\pagebreak

###b)
Using 10 000 draws we seek to comupte the posterior probability Pr(theta < 0.4|y).
```{r task1b, echo = FALSE}
  a = 2
  b = 2
  n = 10000

  posteriorAlpha = a+20*p
  posteriorBeta = b+20*(1-p)
  posteriorRandomDraw <- rbeta(10000, posteriorAlpha, posteriorBeta)
  posteriorRandomOverCondition = which(posteriorRandomDraw < 0.4)
  probabilityCondition = length(posteriorRandomOverCondition)/length(posteriorRandomDraw)
  trueProbability = pbeta(0.4, posteriorAlpha, posteriorBeta)
  
  print(paste0("propability condition with random: ", probabilityCondition))
  print(paste0("ground truth probability: ", trueProbability))

```
Looking at the plot above, the probability for theta < 0.4|y is very small. The simulated value is relatively close to the ground truth. 
(Note: The further to the left on the tail, the larger sample we will need as the data points become more sparse.)

###c)
Computing posterior distribution of the log-odds by simulating 10 000 random draws.

```{r, echo = FALSE}
  a = 2
  b = 2
  n = 10000
  
  posteriorAlpha = a+20*p
  posteriorBeta = b+20*(1-p)
  posteriorRandomDraw <- rbeta(10000, posteriorAlpha, posteriorBeta)
  
  attach(mtcars)
  par(mfrow=c(1,2)) 
  hist(posteriorRandomDraw)
  logOdds = log(posteriorRandomDraw/(1 - posteriorRandomDraw))
  hist(logOdds)
  density(logOdds)

```
\pagebreak

## Task 2
Log-normal distribution and the Gini coefficient
###a)
Simunaltion 10 000 draws from posterior of variance, assuming mean = 3.5 and comparing with the theoretical Inv Chi square posterior distribution.

"Chi posterior of variance" plot is the basis for comparison. The posterior CDF of the random draws together with the histogram of the random posterior shows that the randoms draws looks as expected in their cdf och histogram plots compared to the theoretical pdf, with the given look och the chi posterior pdf the cdf should have a relatively flat surface for a lot of values as a lot of values will be around 0.5, this is also shown when looking at the histogram.
```{r, echo = FALSE, message = FALSE, warning = FALSE}
#library(manipulate)
library(LaplacesDemon)
u = 3.5
y = c(14, 25, 45, 25, 30, 33, 19, 50, 34, 67)
logY = log(y)
meanY = sum(y)/length(y)
meanLogY = sum(logY)/length(logY)

getCdfValue <- function(y, target){
  sortedY = sort(y)
  n = length(sortedY)
  i = 0
  for(dataPoint in sortedY){
    i = i + 1
    if(dataPoint >= target){
      return(i/n)
    }
  }
}

getDistrubutionFromPoints <- function(x){
  n = length(x)
  sortedX = sort(x)
  cumPercent = (1:n)/n
  densityPercent = (cumPercent[2:n] - cumPercent[1:n-1])/(sortedX[2:n] - sortedX[1:(n-1)])
  
  dataMatrix = matrix(1:(2*n), nrow=n, ncol=2, byrow=TRUE)
  dataMatrix[ ,1] = sortedX
  dataMatrix[ ,2] = cumPercent
  
  return(dataMatrix)
}

getIntervalData <- function(x, lowerPercent, upperPercent){
  lowerFlag = FALSE
  upperFlag = FALSE
  n = length(x)
  sortedX = sort(x)
  i = 0
  for(data in x) {
    i = i + 1
    if(i/n >= lowerPercent && !lowerFlag) {
      lowerFlag = TRUE
      lowerIndex = i
    }
    else if(i/n > upperPercent){
      upperIndex = i - 1
      upperFlag = TRUE
      break;
    }
    
  }
  if(!upperFlag){
    upperIndex = n
  }
  return(sortedX[lowerIndex:upperIndex])
}


drawFromPosterior <- function(mean, n, logy, task){
  xGrid <- seq(0.001, 0.999, by=0.001)
  tau2 = (sum(logy - mean)^2)/n
  posterior = dinvchisq(xGrid, n, tau2, log=FALSE)
  
  maxDensity <- max(posterior)
  
  posteriorRandomDraw <- rinvchisq(10000,  n, tau2)
  
  #plotting
  if(task == "a"){
    attach(mtcars)
    par(mfrow=c(2,2))
    plot(xGrid, posterior, type = 'l', lwd = 3, col = "blue", xlim <- c(0,1), ylim <- c(0, maxDensity), 
         xlab = "theta", ylab = 'Density', main = 'Chi Posterior of Variance')
    plot(sort(posteriorRandomDraw), xlab="no. of draws", ylab = "Cumulative probability", 
         main = "CDF posterior random draws")
    hist(posteriorRandomDraw)
  }
  #part b)
  giniTheory = 2*pnorm(posteriorRandomDraw/sqrt(2), mean = 0, sd = 1) - 1
  logGini = log(giniTheory)
  distrubtionGiniLog = getDistrubutionFromPoints(logGini)
  distrubtionGini = getDistrubutionFromPoints(giniTheory)
  if (task == "b"){
    attach(mtcars)
    par(mfrow=c(2,2)) 
    plot(distrubtionGiniLog, xlab = "logGini coef.", ylab = "Cumulative probability", main = "logGIni CDF")
    hist(logGini)
    #plot(distrubtionGini, xlab = "Gini coef.", ylab = "Cumulative probability", main = "Gini CDF")
    #hist(giniTheory)
  }
  #part c)
  if (task == "c"){
    #attach(mtcars)
    lowerData = getIntervalData(giniTheory, 0, 0.025)
    middleData = getIntervalData(giniTheory, 0.025, 0.975)
    upperData = getIntervalData(giniTheory, 0.975, 1)
    
    print(density(middleData))
    print(paste0("Lower end of interval: ", min(middleData)))
    print(paste0("Upper end of interval: ", max(middleData)))
  }
}

#Call task a
num = length(logY)
draws = drawFromPosterior(u, num, logY, task = "a")
```

###b)
Using posterior from a) to compute the posterior distribution of the Gini coefficient for the current dataset.
```{r, echo = FALSE, message = FALSE}
#Call taks b
u = 3.5
y = c(14, 25, 45, 25, 30, 33, 19, 50, 34, 67)
logY = log(y)
meanY = sum(y)/length(y)
meanLogY = sum(logY)/length(logY)
num = length(logY)
draws = drawFromPosterior(u, num, logY, task = "b")
```

###c)
Using posterior draws from b) to calculate a 95% euqal tail credible interval for the Gini coefficient G. In addition a kernel density estimate of the posterior of G to use that kernel density to compute a 95% HPD intercal for G. 
```{r, echo = FALSE, message = FALSE}
#Call taks c
u = 3.5
y = c(14, 25, 45, 25, 30, 33, 19, 50, 34, 67)
logY = log(y)
meanY = sum(y)/length(y)
meanLogY = sum(logY)/length(logY)
num = length(logY)
draws = drawFromPosterior(u, num, logY, task = "c")
```
We can see that the interval for using our cutoff-method is more narrow than the interval for using the density-function as the kernel density estimate provides a interval of[0.006348, 0.074790] compared to the cutoff-interval of [0.01114952, 0.06998829].

\pagebreak

## Task 3
Bayesian inference for the concentration parameter in the von Mises distribution.

###a)
Plot the posterior distribution of k (concentration parameter) for the wind direction.

```{r, echo = FALSE, message = FALSE}
#Part 3
mu = 2.39
obsRad = c(-2.44, 2.14, 2.54, 1.83, 2.02, 2.33, -2.79, 2.23, 2.07, 2.02)

kGrid <- seq(0.001, 10, by=0.001)

priorDist <- dexp(kGrid)
kappa = kGrid

dvonmises <- function(y, mu, kappa){
  res = 1
  for (i in y){
      res = res*(exp(kappa*cos(i-mu))/(2*pi*besselI(kappa,0)))
  }
  return(res)
}

likeliHood <- dvonmises(obsRad, mu, kappa)
posterior = likeliHood*priorDist

posterior = (posterior/sum(posterior))/0.001
likeliHood = (likeliHood/sum(likeliHood))/0.001

plot(kGrid, priorDist, type = 'l', lwd = 3, col = "blue", xlab = "k", 
          ylab = 'Density', main = 'von Mises - Wind direction')
  lines(kGrid, likeliHood, lwd = 3, col = "red")
  lines(kGrid, posterior, lwd = 3, col = "green")
  legend(x = 5, y = 0.8, , legend = c("Likelihood (normalized)", "Prior", "Posterior"), 
        col = c("blue","green","red"), lwd = c(3,3,3), cex = 0.7)    
```

###b)
Approximate posterior mode of the concentration parameter k given the information in a).
```{r, echo = FALSE, message = FALSE}
md = Mode(posterior)
index = which(posterior == max(posterior))
k = kGrid[index]
print(k)
```
Output above shows the approximate mode of posterior, which is expected when comparing to the graph above(the green line).
