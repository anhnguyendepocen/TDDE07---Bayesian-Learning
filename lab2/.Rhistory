randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.1, 0, 0, 0, 0.01, 0, 0, 0, 0.01), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
}
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.1, 0, 0, 0, 0.01, 0, 0, 0, 0.01), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.05, 0, 0, 0, 0.01, 0, 0, 0, 0.01), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.05, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.03, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
}
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
View(womenWork)
View(womenWork)
glmModel <glm(Work ~ 0 + ., data = womenWork, family = binomial)
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
glmModel
View(womenWork)
View(womenWork)
setwd("C:/ML School/TDDE07/labs/codeFromLectures/lec6")
Probit <- 1 # If Probit <-0, then logistic model is used.
chooseCov <- c(1:16) # Here we choose which covariates to include in the model
tau <- 10000; # Prior scaling factor such that Prior Covariance = (tau^2)*I
###########     END USER INPUT    ################
# install.packages("mvtnorm") # Loading a package that contains the multivariate normal pdf
library("mvtnorm") # This command reads the mvtnorm package into R's memory. NOW we can use dmvnorm function.
# Loading data from file
Data<-read.table("SpamReduced.dat",header=TRUE)  # Spam data from Hastie et al.
y <- as.vector(Data[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- as.matrix(Data[,2:17]);
covNames <- names(Data)[2:length(names(Data))];
X <- X[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
View(Sigma)
View(Sigma)
View(X)
View(X)
dim(x)
dim(X)
setwd("C:/ML School/TDDE07/labs/lab2")
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
dim(womenWork)
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
### b
chooseCov <- c(1:8) # Here we choose which covariates to include in the model
tau <- 10; # Prior scaling factor such that Prior Covariance = (tau^2)*I
###########     END USER INPUT    ################
# install.packages("mvtnorm") # Loading a package that contains the multivariate normal pdf
library("mvtnorm") # This command reads the mvtnorm package into R's memory. NOW we can use dmvnorm function.
# Loading data from file
Data<-read.table("SpamReduced.dat",header=TRUE)  # Spam data from Hastie et al.
y <- as.vector(womenWork[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- as.matrix(womenWork[,2:9]);
covNames <- names(womenWork)[2:length(names(womenWork))];
X <- X[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
OptimResults
-solve(OptimResults$hessian)
inv(OptimResults$hessian)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
names(approxPostStd) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
betatilde = OptimResults$par
print("Betatilde": )
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
names(approxPostStd) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
View(X)
View(womenWork)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
### b
chooseCov <- c(1:8) # Here we choose which covariates to include in the model
tau <- 10; # Prior scaling factor such that Prior Covariance = (tau^2)*I
###########     END USER INPUT    ################
# install.packages("mvtnorm") # Loading a package that contains the multivariate normal pdf
library("mvtnorm") # This command reads the mvtnorm package into R's memory. NOW we can use dmvnorm function.
# Loading data from file
Data<-read.table("SpamReduced.dat",header=TRUE)  # Spam data from Hastie et al.
y <- as.vector(womenWork[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- as.matrix(womenWork[,2:9]);
covNames <- names(womenWork)[2:length(names(womenWork))];
X <- X[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
betatilde["NSmallChild"]
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
print("Intervall NSmallChild: ")
upperb = betatilde["NSmallChild"] + 1.64*approxPostStd["NSmallChild"]
lowerb = betatilde["NSmallChild"] - 1.64*approxPostStd["NSmallChild"]
print(upperb)
print(lowerb)
optimResults
OptimResults
covNames
approxPostStd^2
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = betatilde, S = approxPostStd^2)
for(betahat in betas) {
working = ladyInput%*%betahat
workOrNots = c(workOrNots, working)
}
betatilde
matrix(betatilde)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = matrix(approxPostStd)^2)
for(betahat in betas) {
working = ladyInput%*%betahat
workOrNots = c(workOrNots, working)
}
matrix(approxPostStd)^2
covarMatrix = -inv(OptimResults$hessian)
covarMatrix
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(betahat in betas) {
working = ladyInput%*%betahat
workOrNots = c(workOrNots, working)
}
betahat
betas
for(row in 1:nrow(betas)) {
working = ladyInput %*% betahat[row, ]
workOrNots = c(workOrNots, working)
}
for(row in 1:nrow(betas)) {
working = ladyInput %*% betas[row, ]
workOrNots = c(workOrNots, working)
}
betas[1, ]
working
workOrNots
hist(workOrNots, n=30)
covarMatrix = inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = ladyInput %*% betas[row, ]
workOrNots = c(workOrNots, working)
}
hist(workOrNots, n=30)
covarMatrix = -inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = ladyInput %*% betas[row, ]
workOrNots = c(workOrNots, working)
}
hist(workOrNots, n=30)
covarMatrix = -inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = log(ladyInput %*% betas[row, ])
workOrNots = c(workOrNots, working)
}
covarMatrix = -inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = exp(ladyInput %*% betas[row, ])
workOrNots = c(workOrNots, working)
}
hist(workOrNots, n=30)
### part c
covarMatrix = -inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = exp(ladyInput %*% betas[row, ])
workOrNots = c(workOrNots, working)
}
hist(workOrNots, n=30)
library(mvtnorm)
library(readr)
library(matlib)
library(LaplacesDemon)
TempLinkoping <- read_csv("TempLinkoping.csv")
#View(TempLinkoping)
x = TempLinkoping['time']
y = TempLinkoping['temp']
x['time2'] = x^2
x['1'] = x['time']/x['time']
ph = x['time']
#Just to have the betas in right order
x['time'] = x['1']
x['1'] = x['time2']
x['time2'] = ph
matrix_x = data.matrix(x)
matrix_y = data.matrix(y)
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.03, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = matrix(c(1, 1000), c(1:1000)/1000, c(1:1000)/1000^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
lines(xs, ys, col="red")
}
}
ys
xs
xs = matrix(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, matrix(1:1000, 1000, 1)/1000^2)
matrix(1:1000, 1000, 1)/1000
matrix(1, 1000, 1)
matrix(1:1000, 1000, 1)/1000^2
(matrix(1:1000, 1000, 1)/1000)^2
xs = matrix(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
xs = rbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
xs
xs = rbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
xs
xs = cbindX(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
xs
0.333^2
library(mvtnorm)
library(readr)
library(matlib)
library(LaplacesDemon)
TempLinkoping <- read_csv("TempLinkoping.csv")
#View(TempLinkoping)
x = TempLinkoping['time']
y = TempLinkoping['temp']
x['time2'] = x^2
x['1'] = x['time']/x['time']
ph = x['time']
#Just to have the betas in right order
x['time'] = x['1']
x['1'] = x['time2']
x['time2'] = ph
matrix_x = data.matrix(x)
matrix_y = data.matrix(y)
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.03, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
lines(xs, ys, col="red")
}
}
ys = xs%*%randomBeta[k, ]
ys
library(mvtnorm)
library(readr)
library(matlib)
library(LaplacesDemon)
TempLinkoping <- read_csv("TempLinkoping.csv")
#View(TempLinkoping)
x = TempLinkoping['time']
y = TempLinkoping['temp']
x['time2'] = x^2
x['1'] = x['time']/x['time']
ph = x['time']
#Just to have the betas in right order
x['time'] = x['1']
x['1'] = x['time2']
x['time2'] = ph
matrix_x = data.matrix(x)
matrix_y = data.matrix(y)
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.03, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
lines(matrix(1:1000, 1000, 1)/1000, ys, col="red")
}
}
