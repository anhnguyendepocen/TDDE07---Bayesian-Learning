<<<<<<< HEAD
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.1, 0, 0, 0, 0.01, 0, 0, 0, 0.01), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
}
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.1, 0, 0, 0, 0.01, 0, 0, 0, 0.01), 3, 3)
=======
#View(TempLinkoping)
x = TempLinkoping['time']
y = TempLinkoping['temp']
x['time2'] = x^2
x['1'] = x['time']/x['time']
ph = x['time']
#Just to have the betas in right order
x['time'] = x['1']
x['1'] = x['time2']
x['time2'] = ph
matrix_x = data.matrix(x)
matrix_y = data.matrix(y)
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.03, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
<<<<<<< HEAD
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.05, 0, 0, 0, 0.01, 0, 0, 0, 0.01), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
=======
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
lines(matrix(1:1000, 1000, 1)/1000, ys, col="red")
}
}
### part b)
muN = inv(t(matrix_x)%*%matrix_x + omega0)%*%(t(matrix_x)%*%matrix_x%*%betahat + omega0%*%mu0)
omegaN = t(matrix_x)%*%matrix_x + omega0
vN = v0 + 3
vNsigmaN2 = v0*sigmasq0 + (t(matrix_y)%*%matrix_y + t(mu0)%*%omega0%*%mu0 - t(muN)%*%omegaN%*%muN)
randomSigma2 <- rinvchisq(n = 100, df = vN, scale = (vNsigmaN2/vN))
randomBetas <- c()
plot(TempLinkoping, col="black")
sigmas = data.frame(randomSigma2)
y_df = data.frame(matrix(1, 1000, 1))
betas0 = c()
betas1 = c()
betas2 = c()
ibeta = 0
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = muN, S = singleSigma*inv(omegaN))
for(k in 1:10) {
ibeta = ibeta + 1
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
<<<<<<< HEAD
lines(xs, ys, col="red")
}
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.05, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
}
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.03, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
lines(xs, ys, col="red")
}
}
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
View(womenWork)
View(womenWork)
glmModel <glm(Work ~ 0 + ., data = womenWork, family = binomial)
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
glmModel
View(womenWork)
View(womenWork)
setwd("C:/ML School/TDDE07/labs/codeFromLectures/lec6")
Probit <- 1 # If Probit <-0, then logistic model is used.
chooseCov <- c(1:16) # Here we choose which covariates to include in the model
tau <- 10000; # Prior scaling factor such that Prior Covariance = (tau^2)*I
###########     END USER INPUT    ################
# install.packages("mvtnorm") # Loading a package that contains the multivariate normal pdf
library("mvtnorm") # This command reads the mvtnorm package into R's memory. NOW we can use dmvnorm function.
# Loading data from file
Data<-read.table("SpamReduced.dat",header=TRUE)  # Spam data from Hastie et al.
y <- as.vector(Data[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- as.matrix(Data[,2:17]);
covNames <- names(Data)[2:length(names(Data))];
X <- X[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
View(Sigma)
View(Sigma)
View(X)
View(X)
dim(x)
dim(X)
setwd("C:/ML School/TDDE07/labs/lab2")
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
dim(womenWork)
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
### b
chooseCov <- c(1:8) # Here we choose which covariates to include in the model
tau <- 10; # Prior scaling factor such that Prior Covariance = (tau^2)*I
###########     END USER INPUT    ################
# install.packages("mvtnorm") # Loading a package that contains the multivariate normal pdf
library("mvtnorm") # This command reads the mvtnorm package into R's memory. NOW we can use dmvnorm function.
# Loading data from file
Data<-read.table("SpamReduced.dat",header=TRUE)  # Spam data from Hastie et al.
y <- as.vector(womenWork[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- as.matrix(womenWork[,2:9]);
covNames <- names(womenWork)[2:length(names(womenWork))];
X <- X[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
OptimResults
-solve(OptimResults$hessian)
inv(OptimResults$hessian)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
names(approxPostStd) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
betatilde = OptimResults$par
print("Betatilde": )
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
names(approxPostStd) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
View(X)
View(womenWork)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
=======
betas0 = c(betas0, randomBeta[k, 1])
betas1 = c(betas1, randomBeta[k, 2])
betas2 = c(betas2, randomBeta[k, 3])
y_df[paste0("trial", ibeta)] <- data.frame(ys)
}
}
y_df = subset(y_df, select = -c(1) )
mediany = matrix(1, 1000, 1)
lowery = matrix(1, 1000, 1)
uppery = matrix(1, 1000, 1)
for (row in 1:nrow(y_df)) {
mediany[row] = median(as.numeric(as.vector(y_df[row, ])))
lowery[row] = quantile(x = as.numeric(as.vector(y_df[row, ])), probs = 0.025)
uppery[row] = quantile(x = as.numeric(as.vector(y_df[row, ])), probs = 0.975)
}
plot(TempLinkoping, col="black")
lines(xs, mediany, col="blue")
lines(xs, lowery, col="red")
lines(xs, uppery, col="red")
hist(betas0, nclass=30)
hist(betas1, nclass=30)
hist(betas2, nclass=30)
### part c
timeOptimum = -betas1/(2*betas2)
hist(timeOptimum, nclass=30)
### part d
## If there is a higher order but we are more certain that higher order parameters are not needed we can set the omega-values high as it creates a stronger prior
## and the mu-prior values at 0.
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
### b
chooseCov <- c(1:8) # Here we choose which covariates to include in the model
tau <- 10; # Prior scaling factor such that Prior Covariance = (tau^2)*I
###########     END USER INPUT    ################
# install.packages("mvtnorm") # Loading a package that contains the multivariate normal pdf
library("mvtnorm") # This command reads the mvtnorm package into R's memory. NOW we can use dmvnorm function.
# Loading data from file
<<<<<<< HEAD
Data<-read.table("SpamReduced.dat",header=TRUE)  # Spam data from Hastie et al.
=======
Data<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
y <- as.vector(womenWork[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- as.matrix(womenWork[,2:9]);
covNames <- names(womenWork)[2:length(names(womenWork))];
X <- X[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
# Printing the results to the screen
names(OptimResults$par) <- covNames # Naming the coefficient by covariates
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian))) # Computing approximate standard deviations.
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
<<<<<<< HEAD
betatilde["NSmallChild"]
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
=======
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
print("Intervall NSmallChild: ")
upperb = betatilde["NSmallChild"] + 1.64*approxPostStd["NSmallChild"]
lowerb = betatilde["NSmallChild"] - 1.64*approxPostStd["NSmallChild"]
print(upperb)
print(lowerb)
<<<<<<< HEAD
optimResults
OptimResults
covNames
approxPostStd^2
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = betatilde, S = approxPostStd^2)
for(betahat in betas) {
working = ladyInput%*%betahat
workOrNots = c(workOrNots, working)
}
betatilde
matrix(betatilde)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = matrix(approxPostStd)^2)
for(betahat in betas) {
working = ladyInput%*%betahat
workOrNots = c(workOrNots, working)
}
matrix(approxPostStd)^2
covarMatrix = -inv(OptimResults$hessian)
covarMatrix
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(betahat in betas) {
working = ladyInput%*%betahat
workOrNots = c(workOrNots, working)
}
betahat
betas
for(row in 1:nrow(betas)) {
working = ladyInput %*% betahat[row, ]
workOrNots = c(workOrNots, working)
}
for(row in 1:nrow(betas)) {
working = ladyInput %*% betas[row, ]
workOrNots = c(workOrNots, working)
}
betas[1, ]
working
workOrNots
hist(workOrNots, n=30)
covarMatrix = inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = ladyInput %*% betas[row, ]
workOrNots = c(workOrNots, working)
}
hist(workOrNots, n=30)
covarMatrix = -inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = ladyInput %*% betas[row, ]
workOrNots = c(workOrNots, working)
}
hist(workOrNots, n=30)
covarMatrix = -inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = log(ladyInput %*% betas[row, ])
workOrNots = c(workOrNots, working)
}
covarMatrix = -inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = exp(ladyInput %*% betas[row, ])
workOrNots = c(workOrNots, working)
}
hist(workOrNots, n=30)
=======
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
### part c
covarMatrix = -inv(OptimResults$hessian)
ladyInput = c(1, 10, 8, 10, (10/10)^2, 40, 1, 1)
workOrNots = c()
betas = rmvt(n = 1000,mu = matrix(betatilde), S = covarMatrix)
for(row in 1:nrow(betas)) {
working = exp(ladyInput %*% betas[row, ])
workOrNots = c(workOrNots, working)
}
hist(workOrNots, n=30)
<<<<<<< HEAD
=======
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
setwd("~/Desktop/LIU VT 2019/TDDE07/TDDE07---Bayesian-Learning/lab2")
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
womenWork<-read.table("WomenWork.dat",header=TRUE)  # Spam data from Hastie et al.
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
glModel
glmModel
womenWork<-read.table("WomenWork.dat",header=TRUE)
glmModel <- glm(Work ~ 0 + ., data = womenWork, family = binomial)
print(glmModel)
print("Betatilde: ", betatilde)
?sprintf
?printf
?fprint
?print
?lines
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
library(mvtnorm)
library(readr)
library(matlib)
library(LaplacesDemon)
TempLinkoping <- read_csv("TempLinkoping.csv")
#View(TempLinkoping)
x = TempLinkoping['time']
y = TempLinkoping['temp']
x['time2'] = x^2
x['1'] = x['time']/x['time']
ph = x['time']
#Just to have the betas in right order
x['time'] = x['1']
x['1'] = x['time2']
x['time2'] = ph
matrix_x = data.matrix(x)
matrix_y = data.matrix(y)
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.03, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
<<<<<<< HEAD
xs = matrix(c(1, 1000), c(1:1000)/1000, c(1:1000)/1000^2)
=======
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
<<<<<<< HEAD
lines(xs, ys, col="red")
}
}
ys
xs
xs = matrix(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, matrix(1:1000, 1000, 1)/1000^2)
matrix(1:1000, 1000, 1)/1000
matrix(1, 1000, 1)
matrix(1:1000, 1000, 1)/1000^2
(matrix(1:1000, 1000, 1)/1000)^2
xs = matrix(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
xs = rbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
xs
xs = rbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
xs
xs = cbindX(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
xs
0.333^2
=======
lines(matrix(1:1000, 1000, 1)/1000, ys, col=alpha("red", 0.5))
}
}
### part b)
muN = inv(t(matrix_x)%*%matrix_x + omega0)%*%(t(matrix_x)%*%matrix_x%*%betahat + omega0%*%mu0)
omegaN = t(matrix_x)%*%matrix_x + omega0
vN = v0 + 3
vNsigmaN2 = v0*sigmasq0 + (t(matrix_y)%*%matrix_y + t(mu0)%*%omega0%*%mu0 - t(muN)%*%omegaN%*%muN)
randomSigma2 <- rinvchisq(n = 100, df = vN, scale = (vNsigmaN2/vN))
randomBetas <- c()
plot(TempLinkoping, col="black")
sigmas = data.frame(randomSigma2)
y_df = data.frame(matrix(1, 1000, 1))
betas0 = c()
betas1 = c()
betas2 = c()
ibeta = 0
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = muN, S = singleSigma*inv(omegaN))
for(k in 1:10) {
ibeta = ibeta + 1
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
betas0 = c(betas0, randomBeta[k, 1])
betas1 = c(betas1, randomBeta[k, 2])
betas2 = c(betas2, randomBeta[k, 3])
y_df[paste0("trial", ibeta)] <- data.frame(ys)
}
}
y_df = subset(y_df, select = -c(1) )
mediany = matrix(1, 1000, 1)
lowery = matrix(1, 1000, 1)
uppery = matrix(1, 1000, 1)
for (row in 1:nrow(y_df)) {
mediany[row] = median(as.numeric(as.vector(y_df[row, ])))
lowery[row] = quantile(x = as.numeric(as.vector(y_df[row, ])), probs = 0.025)
uppery[row] = quantile(x = as.numeric(as.vector(y_df[row, ])), probs = 0.975)
}
plot(TempLinkoping, col="black")
lines(xs, mediany, col="blue")
lines(xs, lowery, col="red")
lines(xs, uppery, col="red")
hist(betas0, nclass=30)
hist(betas1, nclass=30)
hist(betas2, nclass=30)
### part c
timeOptimum = -betas1/(2*betas2)
hist(timeOptimum, nclass=30)
### part d
## If there is a higher order but we are more certain that higher order parameters are not needed we can set the omega-values high as it creates a stronger prior
## and the mu-prior values at 0.
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
library(mvtnorm)
library(readr)
library(matlib)
library(LaplacesDemon)
TempLinkoping <- read_csv("TempLinkoping.csv")
#View(TempLinkoping)
x = TempLinkoping['time']
y = TempLinkoping['temp']
x['time2'] = x^2
x['1'] = x['time']/x['time']
ph = x['time']
#Just to have the betas in right order
x['time'] = x['1']
x['1'] = x['time2']
x['time2'] = ph
matrix_x = data.matrix(x)
matrix_y = data.matrix(y)
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.03, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
<<<<<<< HEAD
lines(xs, ys, col="red")
}
}
ys = xs%*%randomBeta[k, ]
ys
=======
lines(matrix(1:1000, 1000, 1)/1000, ys, col=alpha("red", 0.5))
}
}
### part b)
muN = inv(t(matrix_x)%*%matrix_x + omega0)%*%(t(matrix_x)%*%matrix_x%*%betahat + omega0%*%mu0)
omegaN = t(matrix_x)%*%matrix_x + omega0
vN = v0 + 3
vNsigmaN2 = v0*sigmasq0 + (t(matrix_y)%*%matrix_y + t(mu0)%*%omega0%*%mu0 - t(muN)%*%omegaN%*%muN)
randomSigma2 <- rinvchisq(n = 100, df = vN, scale = (vNsigmaN2/vN))
randomBetas <- c()
plot(TempLinkoping, col="black")
sigmas = data.frame(randomSigma2)
y_df = data.frame(matrix(1, 1000, 1))
betas0 = c()
betas1 = c()
betas2 = c()
ibeta = 0
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = muN, S = singleSigma*inv(omegaN))
for(k in 1:10) {
ibeta = ibeta + 1
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
betas0 = c(betas0, randomBeta[k, 1])
betas1 = c(betas1, randomBeta[k, 2])
betas2 = c(betas2, randomBeta[k, 3])
y_df[paste0("trial", ibeta)] <- data.frame(ys)
}
}
y_df = subset(y_df, select = -c(1) )
mediany = matrix(1, 1000, 1)
lowery = matrix(1, 1000, 1)
uppery = matrix(1, 1000, 1)
for (row in 1:nrow(y_df)) {
mediany[row] = median(as.numeric(as.vector(y_df[row, ])))
lowery[row] = quantile(x = as.numeric(as.vector(y_df[row, ])), probs = 0.025)
uppery[row] = quantile(x = as.numeric(as.vector(y_df[row, ])), probs = 0.975)
}
plot(TempLinkoping, col="black")
lines(xs, mediany, col="blue")
lines(xs, lowery, col="red")
lines(xs, uppery, col="red")
hist(betas0, nclass=30)
hist(betas1, nclass=30)
hist(betas2, nclass=30)
### part c
timeOptimum = -betas1/(2*betas2)
hist(timeOptimum, nclass=30)
### part d
## If there is a higher order but we are more certain that higher order parameters are not needed we can set the omega-values high as it creates a stronger prior
## and the mu-prior values at 0.
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
library(mvtnorm)
library(readr)
library(matlib)
library(LaplacesDemon)
TempLinkoping <- read_csv("TempLinkoping.csv")
#View(TempLinkoping)
x = TempLinkoping['time']
y = TempLinkoping['temp']
x['time2'] = x^2
x['1'] = x['time']/x['time']
ph = x['time']
#Just to have the betas in right order
x['time'] = x['1']
x['1'] = x['time2']
x['time2'] = ph
matrix_x = data.matrix(x)
matrix_y = data.matrix(y)
mu0 = c(-11,85,-70)
omega0 = matrix(c(0.03, 0, 0, 0, 0.01, 0, 0, 0, 0.03), 3, 3)
v0 = 3
sigmasq0 = 0.03
e = rnorm(1, mean = 0, sd = sigmasq0)
betahat = inv((t(matrix_x)%*%matrix_x))%*%(t(matrix_x)%*%matrix_y)
randomSigma2 <- rinvchisq(n = 10, df = v0, scale = sigmasq0)
randomBetas <- c()
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
<<<<<<< HEAD
lines(matrix(1:1000, 1000, 1)/1000, ys, col="red")
=======
lines(matrix(1:1000, 1000, 1)/1000, ys, col=alpha("red", 0.5))
}
}
### part b)
muN = inv(t(matrix_x)%*%matrix_x + omega0)%*%(t(matrix_x)%*%matrix_x%*%betahat + omega0%*%mu0)
omegaN = t(matrix_x)%*%matrix_x + omega0
vN = v0 + 3
vNsigmaN2 = v0*sigmasq0 + (t(matrix_y)%*%matrix_y + t(mu0)%*%omega0%*%mu0 - t(muN)%*%omegaN%*%muN)
randomSigma2 <- rinvchisq(n = 100, df = vN, scale = (vNsigmaN2/vN))
randomBetas <- c()
plot(TempLinkoping, col="black")
sigmas = data.frame(randomSigma2)
y_df = data.frame(matrix(1, 1000, 1))
betas0 = c()
betas1 = c()
betas2 = c()
ibeta = 0
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = muN, S = singleSigma*inv(omegaN))
for(k in 1:10) {
ibeta = ibeta + 1
ys = c()
xs = c()
for (i in 1:1000) {
y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
ys = c(ys, y)
xs = c(xs, i/1000)
}
betas0 = c(betas0, randomBeta[k, 1])
betas1 = c(betas1, randomBeta[k, 2])
betas2 = c(betas2, randomBeta[k, 3])
y_df[paste0("trial", ibeta)] <- data.frame(ys)
}
}
y_df = subset(y_df, select = -c(1) )
mediany = matrix(1, 1000, 1)
lowery = matrix(1, 1000, 1)
uppery = matrix(1, 1000, 1)
for (row in 1:nrow(y_df)) {
mediany[row] = median(as.numeric(as.vector(y_df[row, ])))
lowery[row] = quantile(x = as.numeric(as.vector(y_df[row, ])), probs = 0.025)
uppery[row] = quantile(x = as.numeric(as.vector(y_df[row, ])), probs = 0.975)
}
plot(TempLinkoping, col="black")
lines(xs, mediany, col="blue")
lines(xs, lowery, col="red")
lines(xs, uppery, col="red")
hist(betas0, nclass=30)
hist(betas1, nclass=30)
hist(betas2, nclass=30)
### part c
timeOptimum = -betas1/(2*betas2)
hist(timeOptimum, nclass=30)
### part d
## If there is a higher order but we are more certain that higher order parameters are not needed we can set the omega-values high as it creates a stronger prior
## and the mu-prior values at 0.
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
lines(matrix(1:1000, 1000, 1)/1000, ys, col=alpha("red", 0.5))
}
}
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
lines(matrix(1:1000, 1000, 1)/1000, ys, col=alpha("red", 0.5))
}
}
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
lines(matrix(1:1000, 1000, 1)/1000, ys, col=alpha(rgb(1,0,0), 0.5))
?alpha
install.packages(scales)
install.packages("scales", dependencies = FALSE)
library(alpha)
library(scales)
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
lines(matrix(1:1000, 1000, 1)/1000, ys, col=alpha(rgb(1,0,0), 0.5))
}
}
plot(TempLinkoping, col="black")
for(singleSigma in randomSigma2) {
randomBeta <- rmvt(n = 10,mu = t(mu0), S = singleSigma*inv(omega0))
for(k in 1:10) {
ys = c()
xs = cbind(matrix(1, 1000, 1), matrix(1:1000, 1000, 1)/1000, (matrix(1:1000, 1000, 1)/1000)^2)
ys = xs%*%randomBeta[k, ]
#for (i in 1:1000) {
#  y = randomBeta[k, 1] + randomBeta[k, 2]*i/1000 + randomBeta[k, 3]*(i/1000)^2
#  ys = c(ys, y)
#  xs = c(xs, i/1000)
#}
lines(matrix(1:1000, 1000, 1)/1000, ys, col='red')
>>>>>>> fff2d55e42386c7f76fbfa79844ec72b95573a0c
}
}
