sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
fit <- stan(file = '8schools.stan', data = schools_dat)
M <- file.path(Sys.getenv("HOME"), ".R", ifelse(.Platform$OS.type == "windows", "Makevars.win", "Makevars"))
file.edit(M)
schools_dat <- list(J = 8,
y = c(28,  8, -3,  7, -1,  1, 18, 12),
sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
fit <- stan(file = '8schools.stan', data = schools_dat)
schools_dat <- list(J = 8,
y = c(28,  8, -3,  7, -1,  1, 18, 12),
sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
fit <- stan(file = '8schools.stan', data = schools_dat)
Sys.getenv("R_MAKEVARS_USER")
schools_dat <- list(J = 8,
y = c(28,  8, -3,  7, -1,  1, 18, 12),
sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
fit <- stan(file = '8schools.stan', data = schools_dat)
schools_dat <- list(J = 8,
y = c(28,  8, -3,  7, -1,  1, 18, 12),
sigma = c(15, 10, 16, 11,  9, 11, 10, 18))
fit <- stan(file = '8schools.stan', data = schools_dat)
print(fit)
sum(c(28,  8, -3,  7, -1,  1, 18, 12))
70/8
plot(fit)
pairs(fit, pars = c("mu", "tau", "lp__"))
schools_dat
la <- extract(fit, permuted = TRUE) # return a list of arrays
mu <- la$mu
a <- extract(fit, permuted = FALSE)
a2 <- as.array(fit)
m <- as.matrix(fit)
d <- as.data.frame(fit)
a2
d
d['eta1']
d['eta[1]']
sum(d['eta[1]'])
sum(d['eta[1]'])/4000
d
install.packages(c("LaplacesDemon", "matlib", "mvtnorm", "readr"))
install.packages(c("LaplacesDemon", "matlib", "mvtnorm", "readr"))
library(mvtnorm)
library(readr)
library(matlib)
library(LaplacesDemon)
rawData <- read.table("ebayNumberOfBidderData.dat",header=TRUE)
x <- as.matrix(rawData)
install.packages('mvtnorm')
install.packages('readr')
library(mvtnorm)
library(readr)
library(matlib)
library(LaplacesDemon)
rawData <- read.table("ebayNumberOfBidderData.dat",header=TRUE)
x <- as.matrix(rawData)
View(x)
View(x)
glm(x)
View(x)
View(x)
View(x)
y <- as.vector(rawData[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- as.matrix(rawData[,2:10]);
library(mvtnorm)
library(readr)
library(matlib)
library(LaplacesDemon)
rawData <- read.table("ebayNumberOfBidderData.dat",header=TRUE)
y <- as.vector(rawData[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- as.matrix(rawData[,2:10]);
model <- glm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10, data=x, family=poisson())
model <- glm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10, data=X, family=poisson())
X <- rawData[,2:10];
model <- glm(y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9+x10, data=X, family=poisson())
X
View(X)
X.columns
X.columns[1]
X[1]
X[,1]
head(X)
View(rawData)
View(X)
model <- glm(y ~ Const+PowerSeller+VerifyID+Sealed+Minblem+Majblem+LargNeg+LogBook+MinBidShare, data=X, family=poisson())
model <- glm(y ~ Const+PowerSeller+VerifyID+Sealed+Minblem+MajBlem+LargNeg+LogBook+MinBidShare, data=X, family=poisson())
summary(model)
y <- as.vector(rawData[,1]); # Data from the read.table function is a data frame. Let's convert y and X to vector and matrix.
X <- rawData[,3:10];
model <- glm(y ~ PowerSeller+VerifyID+Sealed+Minblem+MajBlem+LargNeg+LogBook+MinBidShare, data=X, family=poisson())
summary(model)
transpose
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
chooseCov <- c(1:8)
covNames <- names(rawData)[3:length(names(rawData))];
X <- X[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
initVal
X
X <- rawData[,2:10];
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
chooseCov <- c(1:9)
covNames <- names(rawData)[3:length(names(rawData))];
X <- X[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
betaVect
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
print(betaVect)
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
print(betaVect)
print(X)
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
print(betaVect)
print(X)
linPred <- X%*%t(betaVect);
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,X,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
chooseCov <- c(1:9)
covNames <- names(rawData)[3:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[3:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
LogPostLogistic <- function(betaVect,y,X,mu,Sigma){
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logLik <- sum( linPred*y -log(1 + exp(linPred)));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logPrior <- dmvnorm(betaVect, matrix(0,nPara,1), Sigma, log=TRUE);
return(logLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
OptimResults
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[3:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
PoiPost <- function(betaVect,y,X, mu, SigmaGPrior) {
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - factorial(y));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logBetaPrior <- dmvnorm(betaVect, matrix(0,nPara,1), SigmaGPrior, log=TRUE);
return(logPoiLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
OptimResults
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[2:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
PoiPost <- function(betaVect,y,X, mu, SigmaGPrior) {
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - factorial(y));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logBetaPrior <- dmvnorm(betaVect, matrix(0,nPara,1), SigmaGPrior, log=TRUE);
return(logPoiLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = LogPostLogistic;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
y
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[2:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
PoiPost <- function(betaVect,y,X, mu, SigmaGPrior) {
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - factorial(y));
if (abs(logLik) == Inf) logLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logBetaPrior <- dmvnorm(betaVect, matrix(0,nPara,1), SigmaGPrior, log=TRUE);
return(logPoiLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = PoiPost;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[2:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
PoiPost <- function(betaVect,y,X, mu, SigmaGPrior) {
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - factorial(y));
if (abs(logPoiLik) == Inf) logPoiLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logBetaPrior <- dmvnorm(betaVect, matrix(0,nPara,1), SigmaGPrior, log=TRUE);
return(logPoiLik + logPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = PoiPost;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[2:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
PoiPost <- function(betaVect,y,X, mu, SigmaGPrior) {
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - factorial(y));
if (abs(logPoiLik) == Inf) logPoiLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logBetaPrior <- dmvnorm(betaVect, matrix(0,nPara,1), SigmaGPrior, log=TRUE);
return(logPoiLik + logBetaPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = PoiPost;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - factorial(y));
betaVect = initVal
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - factorial(y));
X
nPara <- length(betaVect);
linPred <- xMatrix%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - factorial(y));
logPoiLik
nPara <- length(betaVect);
linPred <- xMatrix%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - log(factorial(y)));
logPoiLik
factorial(y)
log(factorial(y))
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[2:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
PoiPost <- function(betaVect,y,X, mu, SigmaGPrior) {
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -log(exp(linPred)) - log(factorial(y)));
if (abs(logPoiLik) == Inf) logPoiLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logBetaPrior <- dmvnorm(betaVect, matrix(0,nPara,1), SigmaGPrior, log=TRUE);
return(logPoiLik + logBetaPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = PoiPost;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[2:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
PoiPost <- function(betaVect,y,X, mu, SigmaGPrior) {
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -log(1 + exp(linPred)) - log(factorial(y)));
if (abs(logPoiLik) == Inf) logPoiLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logBetaPrior <- dmvnorm(betaVect, matrix(0,nPara,1), SigmaGPrior, log=TRUE);
return(logPoiLik + logBetaPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = PoiPost;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
OptimResults
y
i = if(y > 0)
i
I = (y > 0)
I
x
X
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[2:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
PoiPost <- function(betaVect,y,X, mu, SigmaGPrior) {
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -exp(linPred) - log(factorial(y)));
print()
if (abs(logPoiLik) == Inf) logPoiLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logBetaPrior <- dmvnorm(betaVect, matrix(0,nPara,1), SigmaGPrior, log=TRUE);
return(logPoiLik + logBetaPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = PoiPost;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
xMatrix = as.matrix(X)
sigmaGPrior = 100*solve((t(xMatrix)%*%xMatrix))
tau = 10
chooseCov <- c(1:9)
covNames <- names(rawData)[2:length(names(rawData))];
xMatrix <- xMatrix[,chooseCov]; # Here we pick out the chosen covariates.
covNames <- covNames[chooseCov];
nPara <- dim(X)[2];
# Setting up the prior
mu <- as.vector(rep(0,nPara)) # Prior mean vector
Sigma <- tau^2*diag(nPara);
PoiPost <- function(betaVect,y,X, mu, SigmaGPrior) {
nPara <- length(betaVect);
linPred <- X%*%betaVect;
logPoiLik <- sum( linPred*y -exp(linPred) - log(factorial(y)));
if (abs(logPoiLik) == Inf) logPoiLik = -20000; # Likelihood is not finite, stear the optimizer away from here!
logBetaPrior <- dmvnorm(betaVect, matrix(0,nPara,1), SigmaGPrior, log=TRUE);
return(logPoiLik + logBetaPrior)
}
initVal <- as.vector(rep(0,dim(X)[2]));
logPost = PoiPost;
OptimResults<-optim(initVal,logPost,gr=NULL,y,xMatrix,mu,Sigma,method=c("BFGS"),control=list(fnscale=-1),hessian=TRUE)
approxPostStd <- sqrt(diag(-solve(OptimResults$hessian)))
names(approxPostStd) <- covNames # Naming the coefficient by covariates
betatilde = OptimResults$par
print("Betatilde: ")
print(betatilde)
print("Jacobiany beta: ")
print(approxPostStd)
